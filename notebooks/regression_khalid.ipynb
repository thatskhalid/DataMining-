{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Assignment\n",
    "In this assignment, we are going to use regression to predict the selling price of a vehicle based on its attributes such as year, mileage, condition. The following features will be considered for our input: odometer, year, condition. These will be our numerical data. We will also look at some categorical data such as make and model. Our target variable will be selling price. We will be utilizing three different regression models on our dataset. They are as follows: linear regression, decision tree regression, and random forest regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/Users/khali/Desktop/Coding Workspace/DataMining/data/cleaned/cleaned_data.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (377860, 822) (377860,)\n",
      "Testing set shape: (94465, 822) (94465,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/Users/khali/Desktop/Coding Workspace/DataMining/data/cleaned/cleaned_data.csv')  \n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data[['odometer', 'year', 'condition', 'make', 'model']]  \n",
    "y = data['sellingprice']  \n",
    "\n",
    "# One-Hot Encode categorical columns in X\n",
    "X = pd.get_dummies(X, columns=['make', 'model'], drop_first=True)  # Modify columns as necessary\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting splits to confirm\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "Train MSE: 17069826.508860268\n",
      "Test MSE: 16499594.093299313\n",
      "Train MAE: 2659.2779756416912\n",
      "Test MAE: 2635.2835306298102\n",
      "Train R^2: 0.8160382999939508\n",
      "Test R^2: 0.8184662661238347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = linear_model.predict(X_train)\n",
    "y_test_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"Train R^2:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regression:\n",
      "Train MSE: 35254419.82503211\n",
      "Test MSE: 35836858.96192157\n",
      "Train MAE: 4020.6561373110503\n",
      "Test MAE: 4036.826267248772\n",
      "Train R^2: 0.620062746368657\n",
      "Test R^2: 0.6057115841175058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the Decision Tree Regressor model\n",
    "tree_model = DecisionTreeRegressor(max_depth=10, random_state=42)  # max_depth can be adjusted\n",
    "\n",
    "# Train the model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = tree_model.predict(X_train)\n",
    "y_test_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nDecision Tree Regression:\")\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"Train R^2:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regression:\n",
      "Train MSE: 33589047.16368291\n",
      "Test MSE: 33988451.93636097\n",
      "Train MAE: 3943.298764313782\n",
      "Test MAE: 3953.1196536843054\n",
      "Train R^2: 0.6380104850739318\n",
      "Test R^2: 0.6260483407174287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "forest_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = forest_model.predict(X_train)\n",
    "y_test_pred = forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nRandom Forest Regression:\")\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"Train R^2:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression:\n",
      "Train MSE: 17069826.508860268\n",
      "Test MSE: 16499594.093299313\n",
      "Train R^2: 0.8160382999939508\n",
      "Test R^2: 0.8184662661238347\n",
      "\n",
      "Decision Tree:\n",
      "Train MSE: 35254419.82503211\n",
      "Test MSE: 35836858.96192157\n",
      "Train R^2: 0.620062746368657\n",
      "Test R^2: 0.6057115841175058\n",
      "\n",
      "Random Forest:\n",
      "Train MSE: 33589047.16368291\n",
      "Test MSE: 33988451.93636097\n",
      "Train R^2: 0.6380104850739318\n",
      "Test R^2: 0.6260483407174287\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Linear Regression\": {\n",
    "        \"Train MSE\": mean_squared_error(y_train, linear_model.predict(X_train)),\n",
    "        \"Test MSE\": mean_squared_error(y_test, linear_model.predict(X_test)),\n",
    "        \"Train R^2\": r2_score(y_train, linear_model.predict(X_train)),\n",
    "        \"Test R^2\": r2_score(y_test, linear_model.predict(X_test)),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Train MSE\": mean_squared_error(y_train, tree_model.predict(X_train)),\n",
    "        \"Test MSE\": mean_squared_error(y_test, tree_model.predict(X_test)),\n",
    "        \"Train R^2\": r2_score(y_train, tree_model.predict(X_train)),\n",
    "        \"Test R^2\": r2_score(y_test, tree_model.predict(X_test)),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Train MSE\": mean_squared_error(y_train, forest_model.predict(X_train)),\n",
    "        \"Test MSE\": mean_squared_error(y_test, forest_model.predict(X_test)),\n",
    "        \"Train R^2\": r2_score(y_train, forest_model.predict(X_train)),\n",
    "        \"Test R^2\": r2_score(y_test, forest_model.predict(X_test)),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print or analyze results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Assignment Questions\n",
    "\n",
    "1) Our input data were the following: odometer, year, condition, make, and model. Our target variable was the selling price.\n",
    "\n",
    "2) We tried the linear regression, decision tree, and random forest models. For the linear regression, the train model had a R-squared value of 0.816 and the test model had a R-squared value of 0.818. Since these values are really similar, we can conclude that this regression model does a good job getting accurate predictions on untrained data. The decision tree regression model did not perform as well and had R-squared values of 0.638 and 0.626 for the training and testing data respectively. This model also had much higher mean squared error values which showcases worse predictions. The random forest model did slightly better than the decision tree model. It had R-squared values of 0.638 for the trainign data and 0.626 for the tetsing data.\n",
    "\n",
    "3) Since the R-squared values for the training and testing data were pretty close, this suggests no overfitting or underfitting occured with the regression model. In regards to the decision tree model, the low R-squared values in addition to the really high mean squared error values can indicate some underfitting, which means that no significant patterns are being analyzed from the data. As for the random forest model, it does predict better than the decision tree model, but not as good as the linear regression model. This can indicate slight underfitting, but not overfitting. \n",
    "\n",
    "4) We're going to try and modify the decision tree model so it does not perform any underfitting. Here is what we did, we increased the max depth from 10 to 20 in order for the model to find more patterns. The model also can now split at multiple nodes and create branches with only one leaf. This should decrease our mean squared error values and increase our R-squared values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Decision Tree Regression:\n",
      "Train MSE: 20206148.47625138\n",
      "Test MSE: 23253333.197020967\n",
      "Train MAE: 2914.5785656696426\n",
      "Test MAE: 3232.1096069530768\n",
      "Train R^2: 0.7822381251305388\n",
      "Test R^2: 0.7441595001396966\n"
     ]
    }
   ],
   "source": [
    "# Question 4 Continued\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "tree_model_complex = DecisionTreeRegressor(\n",
    "    max_depth=20,  # Increase the max depth to capture more complex patterns\n",
    "    min_samples_split=2,  # Allow splits at every node (lowering this increases complexity)\n",
    "    min_samples_leaf=1,  # Allow smaller leaf nodes (increases complexity)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tree_model_complex.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_complex = tree_model_complex.predict(X_train)\n",
    "y_test_pred_complex = tree_model_complex.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nComplex Decision Tree Regression:\")\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred_complex))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred_complex))\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, y_train_pred_complex))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, y_test_pred_complex))\n",
    "print(\"Train R^2:\", r2_score(y_train, y_train_pred_complex))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_test_pred_complex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Assignment Questions Continued \n",
    "\n",
    "4) After modifying the model, we can see a significant difference in our mean squared error and R-squared values. The MSE values for the training data dropped from 35 million to 20 and the testing data dropped from 35 million to 23 million which indicates a more balanced fit. The R-squared values for the training and testing data also increased to 0.782 and 0.744 respectively. Since these values are fairly close, we can conclude that there is no overfitting occuring here as well as no underfitting due to the increased complexity in the pattern analysis leading to better predictions. \n",
    "\n",
    "5) In order to increase the performance of our models from the data perspective, we can do the following: gather more data, clean the data and fine-tune it better, find and determine outliers, try better encoding methods for categorical variables, and maybe incorporate a time-based analysis which can go over the sale date of vehicles to better find trends and patterns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
